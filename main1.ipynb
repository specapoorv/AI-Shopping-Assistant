{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81616790",
   "metadata": {},
   "source": [
    "Person 2) Create a model to run similarity searches on the image directly and find similarities across the web, possibly directly training on some common items or shopping database if possible \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebbede0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: clip-interrogator==0.6.0 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: openai-clip in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: pillow in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (11.2.1)\n",
      "Requirement already satisfied: requests in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: tqdm in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: aiohttp in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (3.11.18)\n",
      "Requirement already satisfied: aiofiles in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (24.1.0)\n",
      "Requirement already satisfied: faiss-cpu in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: python-dotenv in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: torch in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from clip-interrogator==0.6.0) (2.7.0)\n",
      "Requirement already satisfied: torchvision in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from clip-interrogator==0.6.0) (0.22.0)\n",
      "Requirement already satisfied: safetensors in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from clip-interrogator==0.6.0) (0.5.3)\n",
      "Requirement already satisfied: open-clip-torch in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from clip-interrogator==0.6.0) (2.32.0)\n",
      "Requirement already satisfied: accelerate in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from clip-interrogator==0.6.0) (1.7.0)\n",
      "Requirement already satisfied: transformers>=4.27.1 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from clip-interrogator==0.6.0) (4.51.3)\n",
      "Requirement already satisfied: ftfy in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from openai-clip) (6.3.1)\n",
      "Requirement already satisfied: regex in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from openai-clip) (2024.11.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: colorama in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from aiohttp) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from aiohttp) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from aiohttp) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from aiohttp) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from aiohttp) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from aiohttp) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from aiohttp) (1.20.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from faiss-cpu) (2.2.6)\n",
      "Requirement already satisfied: packaging in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: filelock in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from transformers>=4.27.1->clip-interrogator==0.6.0) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from transformers>=4.27.1->clip-interrogator==0.6.0) (0.31.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from transformers>=4.27.1->clip-interrogator==0.6.0) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from transformers>=4.27.1->clip-interrogator==0.6.0) (0.21.1)\n",
      "Requirement already satisfied: psutil in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from accelerate->clip-interrogator==0.6.0) (7.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from torch->clip-interrogator==0.6.0) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from torch->clip-interrogator==0.6.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from torch->clip-interrogator==0.6.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from torch->clip-interrogator==0.6.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from torch->clip-interrogator==0.6.0) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from torch->clip-interrogator==0.6.0) (80.8.0)\n",
      "Requirement already satisfied: wcwidth in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from ftfy->openai-clip) (0.2.13)\n",
      "Requirement already satisfied: timm in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from open-clip-torch->clip-interrogator==0.6.0) (1.0.15)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from sympy>=1.13.3->torch->clip-interrogator==0.6.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\madhav offline\\iitm huge files\\appian\\venv\\lib\\site-packages (from jinja2->torch->clip-interrogator==0.6.0) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install clip-interrogator==0.6.0          \\\n",
    "            openai-clip                       \\\n",
    "            pillow requests tqdm              \\\n",
    "            aiohttp aiofiles                  \\\n",
    "            faiss-cpu                         \\\n",
    "            python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ced305d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Madhav Offline\\IITM Huge Files\\Appian\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading caption model blip-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Madhav Offline\\IITM Huge Files\\Appian\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\madha\\.cache\\huggingface\\hub\\models--Salesforce--blip-image-captioning-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "ERROR:root:Model config for ViT-L not found; available models ['coca_base', 'coca_roberta-ViT-B-32', 'coca_ViT-B-32', 'coca_ViT-L-14', 'convnext_base', 'convnext_base_w', 'convnext_base_w_320', 'convnext_large', 'convnext_large_d', 'convnext_large_d_320', 'convnext_small', 'convnext_tiny', 'convnext_xlarge', 'convnext_xxlarge', 'convnext_xxlarge_320', 'EVA01-g-14', 'EVA01-g-14-plus', 'EVA02-B-16', 'EVA02-E-14', 'EVA02-E-14-plus', 'EVA02-L-14', 'EVA02-L-14-336', 'MobileCLIP-B', 'MobileCLIP-S1', 'MobileCLIP-S2', 'mt5-base-ViT-B-32', 'mt5-xl-ViT-H-14', 'nllb-clip-base', 'nllb-clip-base-siglip', 'nllb-clip-large', 'nllb-clip-large-siglip', 'RN50', 'RN50-quickgelu', 'RN50x4', 'RN50x4-quickgelu', 'RN50x16', 'RN50x16-quickgelu', 'RN50x64', 'RN50x64-quickgelu', 'RN101', 'RN101-quickgelu', 'roberta-ViT-B-32', 'swin_base_patch4_window7_224', 'ViT-B-16', 'ViT-B-16-plus', 'ViT-B-16-plus-240', 'ViT-B-16-quickgelu', 'ViT-B-16-SigLIP', 'ViT-B-16-SigLIP2', 'ViT-B-16-SigLIP2-256', 'ViT-B-16-SigLIP2-384', 'ViT-B-16-SigLIP2-512', 'ViT-B-16-SigLIP-256', 'ViT-B-16-SigLIP-384', 'ViT-B-16-SigLIP-512', 'ViT-B-16-SigLIP-i18n-256', 'ViT-B-32', 'ViT-B-32-256', 'ViT-B-32-plus-256', 'ViT-B-32-quickgelu', 'ViT-B-32-SigLIP2-256', 'ViT-bigG-14', 'ViT-bigG-14-CLIPA', 'ViT-bigG-14-CLIPA-336', 'ViT-bigG-14-quickgelu', 'ViT-e-14', 'ViT-g-14', 'ViT-gopt-16-SigLIP2-256', 'ViT-gopt-16-SigLIP2-384', 'ViT-H-14', 'ViT-H-14-378', 'ViT-H-14-378-quickgelu', 'ViT-H-14-CLIPA', 'ViT-H-14-CLIPA-336', 'ViT-H-14-quickgelu', 'ViT-H-16', 'ViT-L-14', 'ViT-L-14-280', 'ViT-L-14-336', 'ViT-L-14-336-quickgelu', 'ViT-L-14-CLIPA', 'ViT-L-14-CLIPA-336', 'ViT-L-14-quickgelu', 'ViT-L-16', 'ViT-L-16-320', 'ViT-L-16-SigLIP2-256', 'ViT-L-16-SigLIP2-384', 'ViT-L-16-SigLIP2-512', 'ViT-L-16-SigLIP-256', 'ViT-L-16-SigLIP-384', 'ViT-M-16', 'ViT-M-16-alt', 'ViT-M-32', 'ViT-M-32-alt', 'ViT-S-16', 'ViT-S-16-alt', 'ViT-S-32', 'ViT-S-32-alt', 'ViT-SO400M-14-SigLIP', 'ViT-SO400M-14-SigLIP2', 'ViT-SO400M-14-SigLIP2-378', 'ViT-SO400M-14-SigLIP-378', 'ViT-SO400M-14-SigLIP-384', 'ViT-SO400M-16-SigLIP2-256', 'ViT-SO400M-16-SigLIP2-384', 'ViT-SO400M-16-SigLIP2-512', 'ViT-SO400M-16-SigLIP-i18n-256', 'vit_medium_patch16_gap_256', 'vit_relpos_medium_patch16_cls_224', 'ViTamin-B', 'ViTamin-B-LTT', 'ViTamin-L', 'ViTamin-L2', 'ViTamin-L2-256', 'ViTamin-L2-336', 'ViTamin-L2-384', 'ViTamin-L-256', 'ViTamin-L-336', 'ViTamin-L-384', 'ViTamin-S', 'ViTamin-S-LTT', 'ViTamin-XL-256', 'ViTamin-XL-336', 'ViTamin-XL-384', 'xlm-roberta-base-ViT-B-32', 'xlm-roberta-large-ViT-H-14'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model ViT-L/14...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Model config for ViT-L not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m ci_cfg = Config(clip_model_name=\u001b[33m\"\u001b[39m\u001b[33mViT-L/14\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m ci_cfg.apply_low_vram_defaults()\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m ci      = \u001b[43mInterrogator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mci_cfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# 1-b  Plain CLIP encoder (same backbone) for fast batching\u001b[39;00m\n\u001b[32m     36\u001b[39m clip_model, clip_preproc = clip.load(\u001b[33m\"\u001b[39m\u001b[33mViT-L/14\u001b[39m\u001b[33m\"\u001b[39m, device=DEVICE)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Madhav Offline\\IITM Huge Files\\Appian\\venv\\Lib\\site-packages\\clip_interrogator\\clip_interrogator.py:71\u001b[39m, in \u001b[36mInterrogator.__init__\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28mself\u001b[39m.clip_offloaded = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     70\u001b[39m \u001b[38;5;28mself\u001b[39m.load_caption_model()\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_clip_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Madhav Offline\\IITM Huge Files\\Appian\\venv\\Lib\\site-packages\\clip_interrogator\\clip_interrogator.py:105\u001b[39m, in \u001b[36mInterrogator.load_clip_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config.quiet:\n\u001b[32m    103\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading CLIP model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.clip_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28mself\u001b[39m.clip_model, _, \u001b[38;5;28mself\u001b[39m.clip_preprocess = \u001b[43mopen_clip\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_model_and_transforms\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclip_model_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclip_model_pretrained_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfp16\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mfp32\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_model_path\u001b[49m\n\u001b[32m    112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28mself\u001b[39m.clip_model.eval()\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Madhav Offline\\IITM Huge Files\\Appian\\venv\\Lib\\site-packages\\open_clip\\factory.py:502\u001b[39m, in \u001b[36mcreate_model_and_transforms\u001b[39m\u001b[34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, image_mean, image_std, image_interpolation, image_resize_mode, aug_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, load_weights_only, **model_kwargs)\u001b[39m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_model_and_transforms\u001b[39m(\n\u001b[32m    473\u001b[39m         model_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    474\u001b[39m         pretrained: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    492\u001b[39m         **model_kwargs,\n\u001b[32m    493\u001b[39m ):\n\u001b[32m    494\u001b[39m     force_preprocess_cfg = merge_preprocess_kwargs(\n\u001b[32m    495\u001b[39m         {},\n\u001b[32m    496\u001b[39m         mean=image_mean,\n\u001b[32m   (...)\u001b[39m\u001b[32m    499\u001b[39m         resize_mode=image_resize_mode,\n\u001b[32m    500\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m     model = \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_quick_gelu\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_quick_gelu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_custom_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_custom_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_patch_dropout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_patch_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_image_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_image_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_preprocess_cfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_preprocess_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_image\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_hf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_hf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_weights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_weights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m     pp_cfg = PreprocessCfg(**model.visual.preprocess_cfg)\n\u001b[32m    523\u001b[39m     preprocess_train = image_transform_v2(\n\u001b[32m    524\u001b[39m         pp_cfg,\n\u001b[32m    525\u001b[39m         is_train=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    526\u001b[39m         aug_cfg=aug_cfg,\n\u001b[32m    527\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Madhav Offline\\IITM Huge Files\\Appian\\venv\\Lib\\site-packages\\open_clip\\factory.py:315\u001b[39m, in \u001b[36mcreate_model\u001b[39m\u001b[34m(model_name, pretrained, precision, device, jit, force_quick_gelu, force_custom_text, force_patch_dropout, force_image_size, force_preprocess_cfg, pretrained_image, pretrained_hf, cache_dir, output_dict, require_pretrained, load_weights_only, **model_kwargs)\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    314\u001b[39m     logging.error(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mModel config for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found; available models \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlist_models()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mModel config for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m force_quick_gelu:\n\u001b[32m    318\u001b[39m     \u001b[38;5;66;03m# override for use of QuickGELU on non-OpenAI transformer models\u001b[39;00m\n\u001b[32m    319\u001b[39m     model_cfg[\u001b[33m\"\u001b[39m\u001b[33mquick_gelu\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Model config for ViT-L not found."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "quickshop.py â€“ find visually-similar products on the open web\n",
    "--------------------------------------------------------------\n",
    "\n",
    "query.jpg           --->  + CLIP Interrogator  --->  prompt text\n",
    "                            |\n",
    "                            +--> Bing Image Search (q=prompt)\n",
    "                                   |\n",
    "                                   v\n",
    "                            thumbnails + product pages\n",
    "                                   |\n",
    "                            +--> CLIP embed each thumb\n",
    "query.jpg  --CLIP embed-->  |       |\n",
    "                            +-- cosine-sim --> ranked list\n",
    "\"\"\"\n",
    "import os, io, json, asyncio, textwrap, hashlib\n",
    "from pathlib import Path\n",
    "from typing  import List, Tuple\n",
    "\n",
    "import requests, aiohttp, aiofiles, tqdm, numpy as np, faiss, torch\n",
    "from PIL import Image\n",
    "from clip_interrogator import Config, Interrogator         # :contentReference[oaicite:0]{index=0}\n",
    "import clip                                                # OpenAI CLIP\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1)  GLOBALS & MODEL LOAD\n",
    "# ------------------------------------------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1-a  CLIP Interrogator (for caption) â€“ uses BLIP-L + ViT-L/14\n",
    "ci_cfg = Config(clip_model_name=\"ViT-L/14\")\n",
    "ci_cfg.apply_low_vram_defaults()\n",
    "ci = Interrogator(ci_cfg)\n",
    "\n",
    "# 1-b  Plain CLIP encoder (same backbone) for fast batching\n",
    "clip_model, clip_preproc = clip.load(\"ViT-L/14\", device=DEVICE)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2)  HELPERS\n",
    "# ------------------------------------------------------------\n",
    "def caption_image(img_path: Path) -> str:\n",
    "    \"\"\"Return the best text prompt for the given image.\"\"\"\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    return ci.interrogate(img)\n",
    "\n",
    "def embed_pil(im: Image.Image) -> np.ndarray:\n",
    "    \"\"\"Return L2-normalised CLIP embedding from a PIL image.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        tensor = clip_preproc(im).unsqueeze(0).to(DEVICE)\n",
    "        vec    = clip_model.encode_image(tensor)\n",
    "        vec    = vec / vec.norm(dim=-1, keepdim=True)\n",
    "    return vec.cpu().numpy().astype(\"float32\")\n",
    "\n",
    "async def fetch_json(session: aiohttp.ClientSession, url: str) -> dict:\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": os.environ[\"BING_SUBSCRIPTION_KEY\"]}\n",
    "    async with session.get(url, headers=headers) as resp:\n",
    "        resp.raise_for_status()\n",
    "        return await resp.json()\n",
    "\n",
    "async def save_thumb(session: aiohttp.ClientSession,\n",
    "                     url: str, dst: Path) -> Path:\n",
    "    try:\n",
    "        async with session.get(url) as resp:\n",
    "            resp.raise_for_status()\n",
    "            data = await resp.read()\n",
    "        async with aiofiles.open(dst, \"wb\") as f:\n",
    "            await f.write(data)\n",
    "        return dst\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3)  CORE â€“ SEARCH & RE-RANK\n",
    "# ------------------------------------------------------------\n",
    "async def bing_search(prompt: str, top_n: int = 80) -> List[dict]:\n",
    "    \"\"\"Return Bing Image Search results for `prompt`.\"\"\"\n",
    "    endpoint = os.environ[\"BING_ENDPOINT\"].rstrip(\"/\") + \"/v7.0/images/search\"\n",
    "    params   = {\"q\": prompt, \"count\": str(top_n), \"safeSearch\": \"Moderate\"}\n",
    "    async with aiohttp.ClientSession() as sess:\n",
    "        data = await fetch_json(sess, endpoint, params=params)\n",
    "    return data.get(\"value\", [])\n",
    "\n",
    "async def download_thumbs(results: List[dict],\n",
    "                          cache_dir: Path) -> List[Tuple[dict, Path]]:\n",
    "    out = []\n",
    "    async with aiohttp.ClientSession() as sess:\n",
    "        tasks = []\n",
    "        for r in results:\n",
    "            url  = r.get(\"thumbnailUrl\") or r.get(\"contentUrl\")\n",
    "            name = hashlib.md5(url.encode()).hexdigest() + \".jpg\"\n",
    "            dst  = cache_dir / name\n",
    "            tasks.append(save_thumb(sess, url, dst))\n",
    "        for r, t in tqdm.tqdm(zip(results, asyncio.as_completed(tasks)),\n",
    "                              total=len(tasks), desc=\"dl thumbs\"):\n",
    "            img_path = await t\n",
    "            if img_path:\n",
    "                out.append((r, img_path))\n",
    "    return out\n",
    "\n",
    "def rank_by_clip(query_vec: np.ndarray, imgs: List[Tuple[dict, Path]],\n",
    "                 top_k: int = 10) -> List[Tuple[float, dict]]:\n",
    "    dim     = query_vec.shape[1]\n",
    "    index   = faiss.IndexFlatIP(dim)\n",
    "    vectors = []\n",
    "    meta    = []\n",
    "    for r, p in imgs:\n",
    "        try:\n",
    "            vec = embed_pil(Image.open(p))\n",
    "            vectors.append(vec)\n",
    "            meta.append(r)\n",
    "        except Exception:\n",
    "            continue\n",
    "    if not vectors:\n",
    "        return []\n",
    "    x = np.vstack(vectors)\n",
    "    index.add(x)\n",
    "    D, I = index.search(query_vec, top_k)\n",
    "    return [(float(D[0][i]), meta[I[0][i]]) for i in range(len(I[0]))]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4)  DRIVER\n",
    "# ------------------------------------------------------------\n",
    "async def find_similar_products(query_img: str,\n",
    "                                k: int = 10,\n",
    "                                thumb_cache: str = \".thumbs\") -> None:\n",
    "    q_path = Path(query_img)\n",
    "    print(\"ğŸ” Interrogating imageâ€¦\")\n",
    "    prompt = caption_image(q_path)\n",
    "    print(\"ğŸ“ Prompt:\", prompt)\n",
    "\n",
    "    print(\"ğŸ” Querying Bing Image Searchâ€¦\")\n",
    "    results = await bing_search(prompt, top_n=100)\n",
    "\n",
    "    cache_dir = Path(thumb_cache); cache_dir.mkdir(exist_ok=True)\n",
    "    print(f\"ğŸ“¥ Downloading thumbnails ({len(results)} candidates)â€¦\")\n",
    "    img_info = await download_thumbs(results, cache_dir)\n",
    "\n",
    "    print(\"âš–ï¸ Ranking by visual similarityâ€¦\")\n",
    "    q_vec = embed_pil(Image.open(q_path))\n",
    "    ranked = rank_by_clip(q_vec, img_info, top_k=k)\n",
    "\n",
    "    print(\"\\nğŸ† TOP MATCHES\")\n",
    "    for score, r in ranked:\n",
    "        print(f\"{score:5.3f}  {r['hostPageUrl'][:90]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5216b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, asyncio\n",
    "a = argparse.ArgumentParser(description=\"open-web product finder\")\n",
    "a.add_argument(\"image\", help=\"path to query image\")\n",
    "a.add_argument(\"-k\", \"--top_k\", type=int, default=10)\n",
    "args = a.parse_args()\n",
    "asyncio.run(find_similar_products(args.image, args.top_k))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
